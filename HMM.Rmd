
---
title: "Hidden Markov Models for Time Series — Worked Examples"
author: "Donald Zvada / Generated by ChatGPT"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, comment = "#>")
set.seed(2025)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(knitr)
```

# Introduction

This R Markdown document contains executable code demonstrating **Poisson Hidden Markov Models (HMMs)**: parameter transforms, scaled forward likelihood, optimisation-based fitting, forward-backward decoding and Viterbi path extraction.  The low-level functions are pedagogical and mirror implementations found in standard texts (e.g. Zucchini et al.).

# 1. Utility functions (parameter transforms & likelihood)

```{r pn2pw-pw2pn, echo=TRUE}
# Transform natural -> working params
pois.HMM.pn2pw <- function(m, lambda, gamma, delta = NULL, stationary = TRUE) {
  tlambda <- log(lambda)
  if (m == 1) return(tlambda)
  # avoid log(0) by small floor
  G <- pmax(gamma, 1e-12)
  foo <- log(G / diag(G))
  tgamma <- as.vector(foo[!diag(m)])
  if (stationary) {
    tdelta <- NULL
  } else {
    tdelta <- log(delta[-1] / delta[1])
  }
  parvect <- c(tlambda, tgamma, tdelta)
  return(parvect)
}

# Transform working -> natural params
pois.HMM.pw2pn <- function(m, parvect, stationary = TRUE) {
  lambda <- exp(parvect[1:m])
  gamma <- diag(m)
  if (m == 1) return(list(lambda = lambda, gamma = gamma, delta = 1))
  # assume next m*(m-1) elements correspond to off-diagonals by row
  off_idx <- (m+1):(m + m*(m-1))
  gamma_vals <- exp(parvect[off_idx])
  gamma[!gamma] <- gamma_vals
  gamma <- gamma / apply(gamma, 1, sum)
  if (stationary) {
    # solve delta' = delta' gamma  with sum(delta)=1
    # (I - gamma^T + 1)^T delta = 1 -> Solve t(diag(m)-gamma+1) %*% delta = rep(1,m)
    delta <- as.vector(solve(t(diag(m) - gamma + 1), rep(1, m)))
    delta <- delta / sum(delta)
  } else {
    idx2 <- (m + m*(m-1) + 1):(m + m*(m-1) + m - 1)
    foo <- c(1, exp(parvect[idx2]))
    delta <- foo / sum(foo)
  }
  return(list(lambda = lambda, gamma = gamma, delta = delta))
}
```

# 2. Forward recursion with scaling (log-likelihood)

```{r mllk, echo=TRUE}
pois.HMM.mllk <- function(parvect, x, m, stationary = TRUE, ...) {
  if (m == 1) return(-sum(dpois(x, exp(parvect), log = TRUE)))
  n <- length(x)
  pn <- pois.HMM.pw2pn(m, parvect, stationary = stationary)
  # initial probabilities times first observation probabilities
  P1 <- dpois(x[1], pn$lambda)
  foo <- pn$delta * P1
  sumfoo <- sum(foo)
  if (sumfoo <= 0) return(.Machine$double.xmax)
  lscale <- log(sumfoo)
  foo <- foo / sumfoo
  if (n >= 2) {
    for (i in 2:n) {
      if (!is.na(x[i])) {
        P <- dpois(x[i], pn$lambda)
      } else {
        P <- rep(1, m)
      }
      foo <- (foo %*% pn$gamma) * P
      sumfoo <- sum(foo)
      # protect against underflow
      if (sumfoo <= 0) return(.Machine$double.xmax)
      lscale <- lscale + log(sumfoo)
      foo <- foo / sumfoo
    }
  }
  mllk <- -lscale
  return(mllk)
}
```

# 3. Forward-Backward (scaled) for posterior state probs and log-likelihood contributions

```{r forward_backward, echo=TRUE}
forward_scaled <- function(x, lambda, gamma, delta) {
  m <- length(lambda); n <- length(x)
  alpha <- matrix(0, nrow = n, ncol = m)
  scales <- numeric(n)
  P1 <- dpois(x[1], lambda)
  a <- delta * P1
  scales[1] <- sum(a)
  a <- a / scales[1]
  alpha[1, ] <- a
  if (n >= 2) {
    for (t in 2:n) {
      Pt <- dpois(x[t], lambda)
      a <- (a %*% gamma) * Pt
      scales[t] <- sum(a)
      a <- a / scales[t]
      alpha[t, ] <- a
    }
  }
  loglik <- sum(log(scales))
  return(list(alpha = alpha, scales = scales, loglik = loglik))
}

backward_scaled <- function(x, lambda, gamma, delta) {
  m <- length(lambda); n <- length(x)
  beta <- matrix(0, nrow = n, ncol = m)
  beta[n, ] <- rep(1, m)
  if (n >= 2) {
    for (t in (n-1):1) {
      Pt1 <- dpois(x[t+1], lambda)
      b <- (gamma %*% (Pt1 * beta[t+1, ]))
      # scale using forward scales would be done when combining, but we just return raw beta
      beta[t, ] <- b
    }
  }
  return(list(beta = beta))
}
```

# 4. Viterbi algorithm (log-domain)

```{r viterbi, echo=TRUE}
viterbi_path <- function(x, lambda, gamma, delta) {
  n <- length(x); m <- length(lambda)
  # log-domain
  log_gamma <- log(gamma)
  log_delta <- log(delta)
  log_p <- sapply(lambda, function(l) dpois(x, l, log = TRUE))
  # delta_1 * p(x1)
  psi <- matrix(0, nrow = n, ncol = m)
  val <- log_delta + log_p[1, ]
  psi[1, ] <- 0
  for (t in 2:n) {
    next_val <- matrix(NA, nrow = m, ncol = m)
    for (j in 1:m) {
      candidates <- val + log_gamma[, j]
      psi[t, j] <- which.max(candidates)
      next_val[j] <- max(candidates) + log_p[t, j]
    }
    val <- as.numeric(next_val)
  }
  path <- integer(n)
  path[n] <- which.max(val)
  for (t in (n-1):1) {
    path[t] <- psi[t+1, path[t+1]]
  }
  return(path)
}
```

# 5. Simulate synthetic Poisson HMM data

```{r simulate, echo=TRUE}
simulate_poisson_hmm <- function(n = 300, lambda = c(1, 6), gamma = matrix(c(0.9,0.1,0.2,0.8),2,2,byrow=TRUE), delta = NULL) {
  m <- length(lambda)
  if (is.null(delta)) {
    # stationary distribution
    delta <- as.vector(solve(t(diag(m) - gamma + 1), rep(1, m)))
    delta <- delta / sum(delta)
  }
  states <- numeric(n)
  obs <- numeric(n)
  states[1] <- sample(1:m, size = 1, prob = delta)
  obs[1] <- rpois(1, lambda[states[1]])
  for (t in 2:n) {
    states[t] <- sample(1:m, size = 1, prob = gamma[states[t-1], ])
    obs[t] <- rpois(1, lambda[states[t]])
  }
  return(list(x = obs, states = states, lambda = lambda, gamma = gamma, delta = delta))
}

# quick simulate
sim <- simulate_poisson_hmm(n = 300, lambda = c(2, 8), gamma = matrix(c(0.95,0.05,0.15,0.85),2,2,byrow=TRUE))
head(sim$x); head(sim$states)
```

# 6. Fit Poisson HMM via optim()

```{r fit_example, echo=TRUE}
x <- sim$x; n <- length(x); m <- 2; stationary <- TRUE

# starting values
lambda_start <- quantile(x, probs = seq(0,1,length.out = m+1))[-1]
gamma_start <- matrix(0.1, nrow = m, ncol = m); diag(gamma_start) <- 0.8
delta_start <- NULL

start_par <- pois.HMM.pn2pw(m, lambda_start, gamma_start, delta = delta_start, stationary = stationary)

opt <- optim(start_par, fn = pois.HMM.mllk, x = x, m = m, stationary = stationary, method = "BFGS", control = list(maxit = 2000))

opt$convergence
est <- pois.HMM.pw2pn(m, opt$par, stationary = stationary)
est$lambda
est$gamma
est$delta
cat("Log-likelihood:", -opt$value, "\\n")
```

# 7. Decode: forward-backward posterior and Viterbi

```{r decode_and_plot, echo=TRUE, fig.height=4, fig.width=8}
# forward-backward scaled
fwd <- forward_scaled(x, est$lambda, est$gamma, est$delta)
alpha <- fwd$alpha
# backward (raw) -- for local probs we can compute beta scaled by using forward scales
bwd <- backward_scaled(x, est$lambda, est$gamma, est$delta)$beta

# compute smoothed local probabilities via alpha * beta (we must account for scaling)
# compute unscaled beta via backward with scaling using forward scales:
# alternative: compute gamma and use forward scaled alpha and recursive relation to get posterior
# Here we compute posterior via standard forward-backward using log-likelihood trick
# Recompute backward with scaling:
backward_scaled2 <- function(x, lambda, gamma, delta, scales) {
  n <- length(x); m <- length(lambda)
  beta <- matrix(0, nrow = n, ncol = m)
  beta[n, ] <- rep(1, m)
  for (t in (n-1):1) {
    Pt1 <- dpois(x[t+1], lambda)
    b <- (gamma %*% (Pt1 * beta[t+1, ]))
    beta[t, ] <- b / scales[t+1]
  }
  return(beta)
}
beta_scaled <- backward_scaled2(x, est$lambda, est$gamma, est$delta, fwd$scales)
# posterior probabilities
posterior <- alpha * beta_scaled
posterior <- posterior / rowSums(posterior)

# Viterbi path
vpath <- viterbi_path(x, est$lambda, est$gamma, est$delta)

# Plot observations and true/decoded states
df <- data.frame(t = 1:length(x), x = x, true_state = factor(sim$states), viterbi = factor(vpath), p1 = posterior[,1])

p1 <- ggplot(df, aes(x = t, y = x)) +
  geom_line(color = "gray60") +
  geom_point(aes(color = viterbi, shape = viterbi), size = 2) +
  scale_color_brewer(palette = "Dark2") +
  labs(x = "Time", y = "Counts", color = "Viterbi state", shape = "Viterbi state") +
  theme_minimal(base_family = "serif") + theme(legend.position = "bottom", axis.title = element_text(face="bold"))

p2 <- ggplot(df, aes(x = t, y = p1)) +
  geom_line(color = "steelblue", size = 0.9) +
  labs(x = "Time", y = "Posterior Pr(State=1)") +
  theme_minimal(base_family = "serif") + theme(axis.title = element_text(face="bold"))

grid.arrange(p1, p2, nrow = 2)
```

# 8. Model diagnostics & goodness-of-fit

```{r diagnostics, echo=TRUE}
# pseudo-residuals can be used; here we'll compute Pearson residuals by state-weighted means
fitted_means <- posterior %*% matrix(est$lambda, ncol = 1)
pearson_resid <- (x - fitted_means) / sqrt(fitted_means)
acf(pearson_resid, main = "ACF of Pearson residuals")
```

# 9. Applying to real data: daily counts (toy example using AirPassengers rounded)

```{r realdata, echo=TRUE, warning=FALSE, message=FALSE}
# AirPassengers are totals but not ideal Poisson; we demonstrate with rounded values for illustration
data("AirPassengers")
y <- as.integer(round(AirPassengers/10)) # scale down for counts
# fit a 2-state Poisson HMM on a short window to keep runtime small
y_sub <- y[1:100]
fit2 <- fit_poisson_hmm <- function(x, m = 2, stationary = TRUE) {
  lambda_start <- quantile(x, probs = seq(0,1,length.out = m+1))[-1]
  gamma_start <- matrix(0.1, nrow = m, ncol = m); diag(gamma_start) <- 0.8
  if (stationary) delta_start <- NULL else delta_start <- rep(1/m, m)
  start_par <- pois.HMM.pn2pw(m, lambda_start, gamma_start, delta = delta_start, stationary = stationary)
  opt <- optim(start_par, fn = pois.HMM.mllk, x = x, m = m, stationary = stationary, method = "BFGS", control = list(maxit = 1000))
  est <- pois.HMM.pw2pn(m, opt$par, stationary = stationary)
  return(list(parvect = opt$par, loglik = -opt$value, est = est, convergence = opt$convergence))
}
res_real <- fit2(y_sub, m = 2)
res_real$est$lambda
res_real$est$gamma
```

# 10. Extensions and notes

- Replace Poisson emissions with Gaussian, negative-binomial, or other families by changing `dpois` and parameter transforms.  
- For large `m` and long sequences, prefer EM / Baum–Welch or specialized packages (`depmixS4`, `HiddenMarkov`) for speed and stability.
- Consider regularization or multiple random starts to avoid local optima.

---

# Session info

```{r sessioninfo, echo=FALSE}
sessionInfo()
```
